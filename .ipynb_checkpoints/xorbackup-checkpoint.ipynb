{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if activation == \"sigmoid\":\n",
    "            self.output = self.sigmoid(self.calculate_net_input()) \n",
    "        elif activation == \"relu\":\n",
    "            self.output = self.relu(self.calculate_net_input()) \n",
    "        return self.output\n",
    "\n",
    "    def calculate_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def sigmoid(self, net_input):\n",
    "        return 1 / (1 + math.exp(-net_input))\n",
    "\n",
    "    def relu(self, net_input):\n",
    "        return max(0, net_input)\n",
    "\n",
    "    def least_squares_error(self, target):\n",
    "        return 0.5 * (target - self.output) ** 2\n",
    "    \n",
    "    def l1_error(self, target):\n",
    "        return (target - self.output)\n",
    "\n",
    "    # What's next?\n",
    "    # Need to determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    # What is this value?\n",
    "    # ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "\n",
    "    def derivative_error_wrt_net_input(self, target):\n",
    "        return self.derivative_error_wrt_output(target) * self.derivative_output_wrt_net_input()\n",
    "\n",
    "    # Least Squares Error: 1/2 * (tⱼ - yⱼ)^2\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def derivative_error_wrt_output(self, target):\n",
    "        #print(\"derivative_error_wrt_output = \", -1 * (target - self.output))\n",
    "        return -1 * (target - self.output)\n",
    "\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def derivative_output_wrt_net_input(self):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def derivative_net_input_wrt_weight(self, index):\n",
    "        #print(\"derivative_net_input_wrt_weight\", self.inputs[index])\n",
    "        return self.inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Creates a Layer in the network consisting of neurons/units. Each layer has one bias.\n",
    "\n",
    "    N, Integer     : number of neurons/units in the layer\n",
    "    bias, Float    : bias of the layer\n",
    "    neurons, Neuron: list of objects of class Neuron\n",
    "    \"\"\"\n",
    "    def __init__(self, N, bias):\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = random.random()\n",
    "\n",
    "        # initialize empty array of neurons\n",
    "        self.neurons = []\n",
    "\n",
    "        # append new Neurons to array\n",
    "        for i in range(N):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "    \n",
    "    def inspect(self):\n",
    "        print('TOTAL NEURONS:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print('Neuron  :', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "            \n",
    "            \n",
    "    def inspect_datapoint(self, inputs):\n",
    "        print(\"Neurons: %d\" % len(self.neurons))\n",
    "        outputs = []\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(\"Neuron : %d\" % n)\n",
    "            op = self.neurons[n].calculate_output(inputs)\n",
    "            outputs.append(op)\n",
    "            print(\"Output : %f\" % op)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    learning_rate = 0.5 # very important! Too low (0.1) will not adjust weights correctly.\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_bias = None, output_layer_weights = None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = Layer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = Layer(num_outputs, output_layer_bias)\n",
    "        # Let's initialize those weights we got there!\n",
    "        self.initialize_weights_hidden_layer(hidden_layer_weights)\n",
    "        self.initialize_weights_output_layer(output_layer_weights)\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "        \n",
    "    def inspect_datapoint(self, inputs, show_hidden_layer=False):\n",
    "        if show_hidden_layer == True:\n",
    "            print('------')\n",
    "            print('* Inputs: {}'.format(self.num_inputs))\n",
    "            print('------')\n",
    "            print('Hidden Layer')\n",
    "        hidden_layer_outputs = self.hidden_layer.inspect_datapoint(inputs)\n",
    "        print('================== Output Layer ==================')\n",
    "        output_layer_outputs = self.output_layer.inspect_datapoint(hidden_layer_outputs)\n",
    "        \n",
    "        \n",
    "    def initialize_weights_hidden_layer(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "\n",
    "    def initialize_weights_output_layer(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.output_layer.neurons)):\n",
    "            for i in range(len(self.hidden_layer.neurons)):\n",
    "                if output_layer_weights:\n",
    "                    self.output_layer.neurons[h].weights.append(output_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.output_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "    # Helper method in train() (Below)\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Using Stochastic Gradient Descent\n",
    "    # Parameter \"inputs\" is the input to the network.\n",
    "    # Parameter \"target\" is the groundtruth label for the given output.\n",
    "    def train(self, inputs, target, inspect_gradient = False):\n",
    "        self.feed_forward(inputs)\n",
    "\n",
    "        # Step 1: Calculate output neurons derivative \n",
    "        # Calculate ∂E/∂z\n",
    "        derivative_error_wrt_output_layer_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for i in range(len(self.output_layer.neurons)):\n",
    "            #  Calculate ∂E/∂zⱼ\n",
    "            derivative_error_wrt_output_layer_net_input[i] = self.output_layer.neurons[i].derivative_error_wrt_net_input(target[i])\n",
    "\n",
    "        # Step 2: Calculate hidden neurons derivative\n",
    "        # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "        derivative_error_wrt_hidden_layer_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for i in range(len(self.hidden_layer.neurons)):\n",
    "            temp = 0\n",
    "            for j in range(len(self.output_layer.neurons)):\n",
    "                # what is temp?\n",
    "                temp += derivative_error_wrt_output_layer_net_input[j] * self.output_layer.neurons[j].weights[i]\n",
    "\n",
    "            derivative_error_wrt_hidden_layer_net_input[i] = temp * self.hidden_layer.neurons[i].derivative_output_wrt_net_input()\n",
    "\n",
    "        # Step 3: Update output neurons weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for weight_num in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_output_layer_net_input[o] * self.output_layer.neurons[o].derivative_net_input_wrt_weight(weight_num)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Output Neuron \",o, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_output_layer_net_input[o])\n",
    "            self.output_layer.neurons[o].bias -= self.learning_rate * derivative_error_wrt_output_layer_net_input[o]\n",
    "                    \n",
    "\n",
    "        # Step 4: Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for weight_num in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_hidden_layer_net_input[h] * self.hidden_layer.neurons[h].derivative_net_input_wrt_weight(weight_num)\n",
    "                self.hidden_layer.neurons[h].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Hidden Neuron \",h, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_hidden_layer_net_input[h])\n",
    "            self.hidden_layer.neurons[h].bias -= self.learning_rate * derivative_error_wrt_hidden_layer_net_input[h]\n",
    "                    \n",
    "                    \n",
    "    def calculate_total_error(self, training_data):\n",
    "        total_error = 0\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            self.feed_forward(training_x)\n",
    "\n",
    "            for k in range(len(training_output)):\n",
    "                #total_error += self.output_layer.neurons[k].l1_error(training_output[k])\n",
    "                total_error += self.output_layer.neurons[k].least_squares_error(training_output[k])\n",
    "        return total_error\n",
    "    \n",
    "    def test_cases(self, training_data):\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            self.feed_forward(training_x) # trains\n",
    "            self.inspect_datapoint(training_data[i][0])\n",
    "            print(\"Target : [%d]\" % training_output[0])\n",
    "            print(\"Input  : [%d, %d]\" % (training_x[0], training_x[1]))\n",
    "            print(\"==================================================\\n\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(t,error, color='b'):\n",
    "    plt.plot(t, error, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- End of Given Code, Starting of Own Code ---------\n",
    "#\n",
    "# Author      : Louise Y. Lai\n",
    "# Email       : ll2663@nyu.edu\n",
    "# N-Number    : N12709809\n",
    "# Description : The program trains a Neural Network with two hidden layers to 'learn' the XOR operation\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.892844\n",
      "Neuron : 1\n",
      "Output : 0.999967\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.017016\n",
      "Target : [0]\n",
      "Input  : [1, 1]\n",
      "==================================================\n",
      "\n",
      "\n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.079519\n",
      "Neuron : 1\n",
      "Output : 0.973417\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.983544\n",
      "Target : [1]\n",
      "Input  : [1, 0]\n",
      "==================================================\n",
      "\n",
      "\n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.082309\n",
      "Neuron : 1\n",
      "Output : 0.977553\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.983718\n",
      "Target : [1]\n",
      "Input  : [0, 1]\n",
      "==================================================\n",
      "\n",
      "\n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.000929\n",
      "Neuron : 1\n",
      "Output : 0.049883\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.018929\n",
      "Target : [0]\n",
      "Input  : [0, 0]\n",
      "==================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN ME! (1/2)\n",
    "# Build the XOR training set\n",
    "trainingData = [[[1,1], [0]],\n",
    "                [[1,0], [1]],\n",
    "                [[0,1], [1]],\n",
    "                [[0,0], [0]]]\n",
    "\n",
    "# Choose an activation function \n",
    "activation = \"sigmoid\"\n",
    "\n",
    "# Choose number of inputs, hidden layers and outputs\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=2,num_outputs=1)\n",
    "\n",
    "# Choose number of iterations\n",
    "num_iterations = 10000\n",
    "\n",
    "# Train the NN\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1]) \n",
    "\n",
    "# Feed the NN input to produce output\n",
    "NeuralNetwork.test_cases(myNN, trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the output is very close to zero (0.01) and very close to one (0.98) at the right times.\n",
    "# Hurray! We have successfully trained the Neural Network to learn the XOR algorithm using fundamental concepts \n",
    "# such as back propogation, learning rates and activation functions.\n",
    "#\n",
    "# Below, we move on to plotting error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error over 1000 iterations:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyJSRBT6IiJFRKEvIlJEFPoiIkXk/wO/Mr2DsTGXpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2344313c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN ME! (2/2)\n",
    "# Loading Data and training (steps are same as above)\n",
    "trainingData = [[[1,1], [0]],\n",
    "                [[1,0], [1]],\n",
    "                [[0,1], [1]],\n",
    "                [[0,0], [0]]]\n",
    "activation = \"sigmoid\"\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=2,num_outputs=1)\n",
    "num_iterations = 1000\n",
    "\n",
    "average_errors = [] # an array of 1000 errors, corresponding to number of iterations\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1])\n",
    "    average_error = NeuralNetwork.calculate_total_error(myNN, trainingData) / len(trainingData) \n",
    "    average_errors.append(average_error)\n",
    "\n",
    "plot_learning_curves(np.arange(num_iterations), average_errors)\n",
    "print (\"Error over %d iterations:\" % num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR NOTE: \n",
    "# if a sharp 'L' shaped graph appears without tapering, please re-run \n",
    "# the code. It should look sharp AND THEN start decreasing in a curved shape.\n",
    "#\n",
    "# OBSERVATIONS:\n",
    "# We see that as the number of iterations increase (x-axis), the average error decreases (y-axis). The sharpest \n",
    "# error reduction happens in the earliest iterations. Then, it decreases at a gradual but not uniform pace. \n",
    "# As the code is re-run, the shape of the graph changes. Sometimes, the error is dramatically reduced within the \n",
    "# first few iterations, creating a long, almost vertical line in the beginning. Other times, the initial error \n",
    "# reduction is not that great, and error reduction is more gradual throughout the iterations. These observations \n",
    "# can be explained by the random assignment of weights at the beginning. Sometimes, the weights can be randomly \n",
    "# guessed more 'correctly' than others, resulting in a more accurate output and hence produce lower error rates.\n",
    "# \n",
    "# SIDE NOTE:\n",
    "# The author learnt a lot about how learning rates can have a huge effect on the NN's accuracy. When the learning\n",
    "# rate was set low, such as 0.1, the NN was very innacurate. This is becuase low learning rates do not 'punish'\n",
    "# errors harshly enough during back-propogation. When the learning rate was set higher, such as 0.5, the NN\n",
    "# works well and is able to achieve near-perfect results. The downside to this is the possibility of overfitting, \n",
    "# but since XOR is limited to a relatively simple exercise with just four possible test cases, this is not \n",
    "# a cause of concern.\n",
    "#\n",
    "#\n",
    "# CONCLUSION:\n",
    "# Overall, plotting this error graph shows that the NN is highly successfull at the XOR operation. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
