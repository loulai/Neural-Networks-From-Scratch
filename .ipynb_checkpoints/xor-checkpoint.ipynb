{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if activation == \"sigmoid\":\n",
    "            self.output = self.sigmoid(self.calculate_net_input()) \n",
    "        elif activation == \"relu\":\n",
    "            self.output = self.relu(self.calculate_net_input()) \n",
    "        return self.output\n",
    "\n",
    "    def calculate_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def sigmoid(self, net_input):\n",
    "        return 1 / (1 + math.exp(-net_input))\n",
    "\n",
    "    def relu(self, net_input):\n",
    "        return max(0, net_input)\n",
    "\n",
    "    def least_squares_error(self, target):\n",
    "        return 0.5 * (target - self.output) ** 2\n",
    "    \n",
    "    def l1_error(self, target):\n",
    "        return (target - self.output)\n",
    "\n",
    "    # What's next?\n",
    "    # Need to determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    # What is this value?\n",
    "    # ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "\n",
    "    def derivative_error_wrt_net_input(self, target):\n",
    "        return self.derivative_error_wrt_output(target) * self.derivative_output_wrt_net_input()\n",
    "\n",
    "    # Least Squares Error: 1/2 * (tⱼ - yⱼ)^2\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def derivative_error_wrt_output(self, target):\n",
    "        #print(\"derivative_error_wrt_output = \", -1 * (target - self.output))\n",
    "        return -1 * (target - self.output)\n",
    "\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def derivative_output_wrt_net_input(self):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def derivative_net_input_wrt_weight(self, index):\n",
    "        #print(\"derivative_net_input_wrt_weight\", self.inputs[index])\n",
    "        return self.inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Creates a Layer in the network consisting of neurons/units. Each layer has one bias.\n",
    "\n",
    "    N, Integer     : number of neurons/units in the layer\n",
    "    bias, Float    : bias of the layer\n",
    "    neurons, Neuron: list of objects of class Neuron\n",
    "    \"\"\"\n",
    "    def __init__(self, N, bias):\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = random.random()\n",
    "\n",
    "        # initialize empty array of neurons\n",
    "        self.neurons = []\n",
    "\n",
    "        # append new Neurons to array\n",
    "        for i in range(N):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "    \n",
    "    def inspect(self):\n",
    "        print('TOTAL NEURONS:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print('Neuron  :', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "            \n",
    "            \n",
    "    def inspect_datapoint(self, inputs):\n",
    "        print(\"Neurons: %d\" % len(self.neurons))\n",
    "        outputs = []\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(\"Neuron : %d\" % n)\n",
    "            op = self.neurons[n].calculate_output(inputs)\n",
    "            outputs.append(op)\n",
    "            print(\"Output : %f\" % op)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    learning_rate = 0.5 # very important! Too low (0.1) will not adjust weights correctly.\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_bias = None, output_layer_weights = None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = Layer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = Layer(num_outputs, output_layer_bias)\n",
    "        # Let's initialize those weights we got there!\n",
    "        self.initialize_weights_hidden_layer(hidden_layer_weights)\n",
    "        self.initialize_weights_output_layer(output_layer_weights)\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "        \n",
    "    def inspect_datapoint(self, inputs, show_hidden_layer=False):\n",
    "        if show_hidden_layer == True:\n",
    "            print('------')\n",
    "            print('* Inputs: {}'.format(self.num_inputs))\n",
    "            print('------')\n",
    "            print('Hidden Layer')\n",
    "        hidden_layer_outputs = self.hidden_layer.inspect_datapoint(inputs)\n",
    "        print('================== Output Layer ==================')\n",
    "        output_layer_outputs = self.output_layer.inspect_datapoint(hidden_layer_outputs)\n",
    "        print(\"==================================================\\n\\n\")  \n",
    "        \n",
    "        \n",
    "    def initialize_weights_hidden_layer(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "\n",
    "    def initialize_weights_output_layer(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.output_layer.neurons)):\n",
    "            for i in range(len(self.hidden_layer.neurons)):\n",
    "                if output_layer_weights:\n",
    "                    self.output_layer.neurons[h].weights.append(output_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.output_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "    # Helper method in train() (Below)\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Using Stochastic Gradient Descent\n",
    "    # Parameter \"inputs\" is the input to the network.\n",
    "    # Parameter \"target\" is the groundtruth label for the given output.\n",
    "    def train(self, inputs, target, inspect_gradient = False):\n",
    "        self.feed_forward(inputs)\n",
    "\n",
    "        # Step 1: Calculate output neurons derivative \n",
    "        # Calculate ∂E/∂z\n",
    "        derivative_error_wrt_output_layer_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for i in range(len(self.output_layer.neurons)):\n",
    "            #  Calculate ∂E/∂zⱼ\n",
    "            derivative_error_wrt_output_layer_net_input[i] = self.output_layer.neurons[i].derivative_error_wrt_net_input(target[i])\n",
    "\n",
    "        # Step 2: Calculate hidden neurons derivative\n",
    "        # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "        derivative_error_wrt_hidden_layer_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for i in range(len(self.hidden_layer.neurons)):\n",
    "            temp = 0\n",
    "            for j in range(len(self.output_layer.neurons)):\n",
    "                # what is temp?\n",
    "                temp += derivative_error_wrt_output_layer_net_input[j] * self.output_layer.neurons[j].weights[i]\n",
    "\n",
    "            derivative_error_wrt_hidden_layer_net_input[i] = temp * self.hidden_layer.neurons[i].derivative_output_wrt_net_input()\n",
    "\n",
    "        # Step 3: Update output neurons weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for weight_num in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_output_layer_net_input[o] * self.output_layer.neurons[o].derivative_net_input_wrt_weight(weight_num)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Output Neuron \",o, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_output_layer_net_input[o])\n",
    "            self.output_layer.neurons[o].bias -= self.learning_rate * derivative_error_wrt_output_layer_net_input[o]\n",
    "                    \n",
    "\n",
    "        # Step 4: Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for weight_num in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_hidden_layer_net_input[h] * self.hidden_layer.neurons[h].derivative_net_input_wrt_weight(weight_num)\n",
    "                self.hidden_layer.neurons[h].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Hidden Neuron \",h, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_hidden_layer_net_input[h])\n",
    "            self.hidden_layer.neurons[h].bias -= self.learning_rate * derivative_error_wrt_hidden_layer_net_input[h]\n",
    "                    \n",
    "                    \n",
    "    def calculate_total_error(self, training_data):\n",
    "        total_error = 0\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            self.feed_forward(training_x)\n",
    "\n",
    "            for k in range(len(training_output)):\n",
    "                #total_error += self.output_layer.neurons[k].l1_error(training_output[k])\n",
    "                total_error += self.output_layer.neurons[k].least_squares_error(training_output[k])\n",
    "        return total_error\n",
    "    \n",
    "    def test_cases(self, training_data):\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            print(\"Input : [%d, %d]\" % (training_x[0], training_x[1]))\n",
    "            print(\"Target: [%d]\" % training_output[0])\n",
    "            print(\"------ \")\n",
    "            self.feed_forward(training_x) # trains\n",
    "            self.inspect_datapoint(training_data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(t,error, color='b'):\n",
    "    plt.plot(t, error, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- End of Given Code, Starting of Own Code ---------\n",
    "#\n",
    "# Author      : Louise Y. Lai\n",
    "# Email       : ll2663@nyu.edu\n",
    "# N-Number    : N12709809\n",
    "# Description : The program trains a Neural Network with two hidden layers to 'learn' the XOR operation\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : [1, 1]\n",
      "Target: [0]\n",
      "------ \n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.896121\n",
      "Neuron : 1\n",
      "Output : 0.999959\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.017011\n",
      "==================================================\n",
      "\n",
      "\n",
      "Input : [1, 0]\n",
      "Target: [1]\n",
      "------ \n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.077518\n",
      "Neuron : 1\n",
      "Output : 0.973105\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.983565\n",
      "==================================================\n",
      "\n",
      "\n",
      "Input : [0, 1]\n",
      "Target: [1]\n",
      "------ \n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.078205\n",
      "Neuron : 1\n",
      "Output : 0.974052\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.983597\n",
      "==================================================\n",
      "\n",
      "\n",
      "Input : [0, 0]\n",
      "Target: [0]\n",
      "------ \n",
      "Neurons: 2\n",
      "Neuron : 0\n",
      "Output : 0.000826\n",
      "Neuron : 1\n",
      "Output : 0.052899\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.019010\n",
      "==================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN ME! (1/2)\n",
    "# Build the XOR training set\n",
    "trainingData = [[[1,1], [0]],\n",
    "                [[1,0], [1]],\n",
    "                [[0,1], [1]],\n",
    "                [[0,0], [0]]]\n",
    "\n",
    "# Choose an activation function \n",
    "activation = \"sigmoid\"\n",
    "\n",
    "# Choose number of inputs, hidden layers and outputs\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=2,num_outputs=1)\n",
    "\n",
    "# Choose number of iterations\n",
    "num_iterations = 10000\n",
    "\n",
    "# Train the NN\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1]) \n",
    "\n",
    "# Feed the NN input to produce output\n",
    "NeuralNetwork.test_cases(myNN, trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the output is very close to zero (0.01) and very close to one (0.98) when at the right times.\n",
    "# Hurray! We have successfully trained the Neural Network to learn the XOR algorithm using fundamental concepts \n",
    "# such as back propogation, learning rates and activation functions.\n",
    "#\n",
    "# Below, we move on to plotting error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error over 1000 iterations:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUTyiEJfRCSPKPRFRPLI/wGVeQNcbJQbgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0f128d650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN ME! (2/2)\n",
    "# Loading Data and training (steps are same as above)\n",
    "trainingData = [[[1,1], [0]],\n",
    "                [[1,0], [1]],\n",
    "                [[0,1], [1]],\n",
    "                [[0,0], [0]]]\n",
    "activation = \"sigmoid\"\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=2,num_outputs=1)\n",
    "num_iterations = 1000\n",
    "\n",
    "average_errors = [] # an array of 1000 errors, corresponding to number of iterations\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1])\n",
    "    average_error = NeuralNetwork.calculate_total_error(myNN, trainingData) / len(trainingData) \n",
    "    average_errors.append(average_error)\n",
    "\n",
    "plot_learning_curves(np.arange(num_iterations), average_errors)\n",
    "print (\"Error over %d iterations:\" % num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# if a sharp 'L' shaped graph appears, please re-run the code. It should look sharp AND THEN decreasing.\n",
    "#\n",
    "# OBSERVATIONS:\n",
    "# We see that as the number of iterations increase (x-axis), the average error decreases (y-axis). The sharpest \n",
    "# error reduction happens in the earliest iterations. Then, it decreases at a gradual but not at a uniform pace. \n",
    "# As the code is re-run, the shape of the graph changes. Sometimes, the error is dramatically reduced within the \n",
    "# first few iterations, creating a long, almost vertical line in the beginning. Other times, the initial error \n",
    "# reduction is not that great, and error reduction is more gradual throughout the iterations. These observations \n",
    "# can be explained by the random assignment of weights at the beginning. Sometimes, the weights can be randomly \n",
    "# guessed more 'correctly' than others, resulting in a more accurate output and hence produce lower error rates.\n",
    "# \n",
    "# SIDE NOTE:\n",
    "# The author learnt a lot about how learning rates can have a huge effect on the NN's accuracy. When the learning\n",
    "# rate was set low, such as 0.1, the NN was very innacurate. This is becuase low learning rates do not 'punish'\n",
    "# errors harshly enough during back-propogation. When the learning rate was set higher, such as 0.5, the NN\n",
    "# works well and is able to achieve near-perfect results. The downside to this is the possibility of overfitting, \n",
    "# but since XOR is limited to a relatively simple exercise with just four possible test cases, this is not \n",
    "# a cause of concern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
