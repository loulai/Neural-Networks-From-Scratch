{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, bias):\n",
    "        self.bias = bias\n",
    "        self.weights = []\n",
    "\n",
    "    def calculate_output(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        if activation == \"sigmoid\":\n",
    "            self.output = self.sigmoid(self.calculate_net_input()) \n",
    "        elif activation == \"relu\":\n",
    "            self.output = self.relu(self.calculate_net_input()) \n",
    "        return self.output\n",
    "\n",
    "    def calculate_net_input(self):\n",
    "        total = 0\n",
    "        for i in range(len(self.inputs)):\n",
    "            total += self.inputs[i] * self.weights[i]\n",
    "        return total + self.bias\n",
    "\n",
    "    def sigmoid(self, net_input):\n",
    "        return 1 / (1 + math.exp(-net_input))\n",
    "\n",
    "    def relu(self, net_input):\n",
    "        return max(0, net_input)\n",
    "\n",
    "    def least_squares_error(self, target):\n",
    "        return 0.5 * (target - self.output) ** 2\n",
    "    \n",
    "    def l1_error(self, target):\n",
    "        return (target - self.output)\n",
    "\n",
    "    # What's next?\n",
    "    # Need to determine how much the neuron's total input has to change to move closer to the expected output\n",
    "    # What is this value?\n",
    "    # ∂E/∂zⱼ = ∂E/∂yⱼ * dyⱼ/dzⱼ\n",
    "\n",
    "    def derivative_error_wrt_net_input(self, target):\n",
    "        return self.derivative_error_wrt_output(target) * self.derivative_output_wrt_net_input()\n",
    "\n",
    "    # Least Squares Error: 1/2 * (tⱼ - yⱼ)^2\n",
    "    # The partial derivate of the error with respect to actual output then is calculated by:\n",
    "    # ∂E/∂yⱼ = -(tⱼ - yⱼ)\n",
    "    def derivative_error_wrt_output(self, target):\n",
    "        #print(\"derivative_error_wrt_output = \", -1 * (target - self.output))\n",
    "        return -1 * (target - self.output)\n",
    "\n",
    "    # dyⱼ/dzⱼ = yⱼ * (1 - yⱼ)\n",
    "    def derivative_output_wrt_net_input(self):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self.output * (1 - self.output)\n",
    "\n",
    "    # The total net input is the weighted sum of all the inputs to the neuron and their respective weights:\n",
    "    # zⱼ = netⱼ = x₁w₁ + x₂w₂ ...\n",
    "    # The partial derivative of the total net input with respective to a given weight (with everything else held constant) then is:\n",
    "    # = ∂zⱼ/∂wᵢ = some constant + 1 * xᵢw₁^(1-0) + some constant ... = xᵢ\n",
    "    def derivative_net_input_wrt_weight(self, index):\n",
    "        #print(\"derivative_net_input_wrt_weight\", self.inputs[index])\n",
    "        return self.inputs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Creates a Layer in the network consisting of neurons/units. Each layer has one bias.\n",
    "\n",
    "    N, Integer     : number of neurons/units in the layer\n",
    "    bias, Float    : bias of the layer\n",
    "    neurons, Neuron: list of objects of class Neuron\n",
    "    \"\"\"\n",
    "    def __init__(self, N, bias):\n",
    "        if bias:\n",
    "            self.bias = bias\n",
    "        else:\n",
    "            self.bias = random.random()\n",
    "\n",
    "        # initialize empty array of neurons\n",
    "        self.neurons = []\n",
    "\n",
    "        # append new Neurons to array\n",
    "        for i in range(N):\n",
    "            self.neurons.append(Neuron(self.bias))\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "\n",
    "    def get_outputs(self):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output)\n",
    "        return outputs\n",
    "    \n",
    "    def inspect(self):\n",
    "        print('TOTAL NEURONS:', len(self.neurons))\n",
    "        for n in range(len(self.neurons)):\n",
    "            print('Neuron  :', n)\n",
    "            for w in range(len(self.neurons[n].weights)):\n",
    "                print('  Weight:', self.neurons[n].weights[w])\n",
    "            print('  Bias:', self.bias)\n",
    "            \n",
    "            \n",
    "    def inspect_datapoint(self, inputs):\n",
    "        print(\"Neurons: %d\" % len(self.neurons))\n",
    "        outputs = []\n",
    "        for n in range(len(self.neurons)):\n",
    "            print(\"Neuron : %d\" % n)\n",
    "            op = self.neurons[n].calculate_output(inputs)\n",
    "            outputs.append(op)\n",
    "            print(\"Output : %f\" % op)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    learning_rate = 0.5 # very important! Too low (0.1) will not adjust weights correctly.\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, hidden_layer_weights = None, hidden_layer_bias = None, output_layer_bias = None, output_layer_weights = None):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_layer = Layer(num_hidden, hidden_layer_bias)\n",
    "        self.output_layer = Layer(num_outputs, output_layer_bias)\n",
    "        # Let's initialize those weights we got there!\n",
    "        self.initialize_weights_hidden_layer(hidden_layer_weights)\n",
    "        self.initialize_weights_output_layer(output_layer_weights)\n",
    "\n",
    "    def inspect(self):\n",
    "        print('------')\n",
    "        print('* Inputs: {}'.format(self.num_inputs))\n",
    "        print('------')\n",
    "        print('Hidden Layer')\n",
    "        self.hidden_layer.inspect()\n",
    "        print('------')\n",
    "        print('* Output Layer')\n",
    "        self.output_layer.inspect()\n",
    "        print('------')\n",
    "        \n",
    "    def inspect_datapoint(self, inputs, show_hidden_layer=False):\n",
    "        if show_hidden_layer == True:\n",
    "            print('------')\n",
    "            print('* Inputs: {}'.format(self.num_inputs))\n",
    "            print('------')\n",
    "            print('Hidden Layer')\n",
    "        hidden_layer_outputs = self.hidden_layer.inspect_datapoint(inputs)\n",
    "        print('================== Output Layer ==================')\n",
    "        output_layer_outputs = self.output_layer.inspect_datapoint(hidden_layer_outputs)\n",
    "        \n",
    "        \n",
    "    def initialize_weights_hidden_layer(self, hidden_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for i in range(self.num_inputs):\n",
    "                if hidden_layer_weights:\n",
    "                    self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.hidden_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "\n",
    "    def initialize_weights_output_layer(self, output_layer_weights):\n",
    "        weight_num = 0\n",
    "        for h in range(len(self.output_layer.neurons)):\n",
    "            for i in range(len(self.hidden_layer.neurons)):\n",
    "                if output_layer_weights:\n",
    "                    self.output_layer.neurons[h].weights.append(output_layer_weights[weight_num])\n",
    "                else:\n",
    "                    self.output_layer.neurons[h].weights.append(random.random())\n",
    "                weight_num += 1\n",
    "\n",
    "    # Helper method in train() (Below)\n",
    "    def feed_forward(self, inputs):\n",
    "        hidden_layer_outputs = self.hidden_layer.feed_forward(inputs)\n",
    "        return self.output_layer.feed_forward(hidden_layer_outputs)\n",
    "\n",
    "    # Using Stochastic Gradient Descent\n",
    "    # Parameter \"inputs\" is the input to the network.\n",
    "    # Parameter \"target\" is the groundtruth label for the given output.\n",
    "    def train(self, inputs, target, inspect_gradient = False):\n",
    "        self.feed_forward(inputs)\n",
    "\n",
    "        # Step 1: Calculate output neurons derivative \n",
    "        # Calculate ∂E/∂z\n",
    "        derivative_error_wrt_output_layer_net_input = [0] * len(self.output_layer.neurons)\n",
    "        for i in range(len(self.output_layer.neurons)):\n",
    "            #  Calculate ∂E/∂zⱼ\n",
    "            derivative_error_wrt_output_layer_net_input[i] = self.output_layer.neurons[i].derivative_error_wrt_net_input(target[i])\n",
    "\n",
    "        # Step 2: Calculate hidden neurons derivative\n",
    "        # dE/dyⱼ = Σ ∂E/∂zⱼ * ∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ\n",
    "        derivative_error_wrt_hidden_layer_net_input = [0] * len(self.hidden_layer.neurons)\n",
    "        for i in range(len(self.hidden_layer.neurons)):\n",
    "            temp = 0\n",
    "            for j in range(len(self.output_layer.neurons)):\n",
    "                temp += derivative_error_wrt_output_layer_net_input[j] * self.output_layer.neurons[j].weights[i]\n",
    "\n",
    "            derivative_error_wrt_hidden_layer_net_input[i] = temp * self.hidden_layer.neurons[i].derivative_output_wrt_net_input()\n",
    "\n",
    "        # Step 3: Update output neurons weights\n",
    "        for o in range(len(self.output_layer.neurons)):\n",
    "            for weight_num in range(len(self.output_layer.neurons[o].weights)):\n",
    "                # ∂Eⱼ/∂wᵢⱼ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_output_layer_net_input[o] * self.output_layer.neurons[o].derivative_net_input_wrt_weight(weight_num)\n",
    "                # Δw = α * ∂Eⱼ/∂wᵢ\n",
    "                self.output_layer.neurons[o].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Output Neuron \",o, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_output_layer_net_input[o])\n",
    "            self.output_layer.neurons[o].bias -= self.learning_rate * derivative_error_wrt_output_layer_net_input[o]\n",
    "                    \n",
    "\n",
    "        # Step 4: Update hidden neuron weights\n",
    "        for h in range(len(self.hidden_layer.neurons)):\n",
    "            for weight_num in range(len(self.hidden_layer.neurons[h].weights)):\n",
    "                # ∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ\n",
    "                derivative_error_wrt_weight = derivative_error_wrt_hidden_layer_net_input[h] * self.hidden_layer.neurons[h].derivative_net_input_wrt_weight(weight_num)\n",
    "                self.hidden_layer.neurons[h].weights[weight_num] -= self.learning_rate * derivative_error_wrt_weight\n",
    "                if inspect_gradient:\n",
    "                    print(\" Hidden Neuron \",h, \"weight \",weight_num, \"gradient: \",derivative_error_wrt_weight, \"del error/net_input\", derivative_error_wrt_hidden_layer_net_input[h])\n",
    "            self.hidden_layer.neurons[h].bias -= self.learning_rate * derivative_error_wrt_hidden_layer_net_input[h]\n",
    "                    \n",
    "                    \n",
    "    def calculate_total_error(self, training_data):\n",
    "        total_error = 0\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            self.feed_forward(training_x)\n",
    "\n",
    "            for k in range(len(training_output)):\n",
    "                #total_error += self.output_layer.neurons[k].l1_error(training_output[k])\n",
    "                total_error += self.output_layer.neurons[k].least_squares_error(training_output[k])\n",
    "        return total_error\n",
    "    \n",
    "    def test_cases(self, training_data):\n",
    "        for i in range(len(training_data)):\n",
    "            training_x, training_output = training_data[i]\n",
    "            print(\"------ \")\n",
    "            self.feed_forward(training_x) # trains\n",
    "            self.inspect_datapoint(training_data[i][0])\n",
    "            print(\"Target : [%d]\" % training_output[0])\n",
    "            print(\"Input  : [%d, %d]\" % (training_x[0], training_x[1]))\n",
    "            print(\"==================================================\\n\\n\")  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(t,error, color='b'):\n",
    "    plt.plot(t, error, color = color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- End of Given Code, Starting of Own Code ---------\n",
    "#\n",
    "# Author      : Louise Y. Lai\n",
    "# Email       : ll2663@nyu.edu\n",
    "# N-Number    : N12709809\n",
    "# Description : The program trains a Neural Network with two hidden layers to 'learn' the XOR operation\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999459\n",
      "Neuron : 1\n",
      "Output : 0.997933\n",
      "Neuron : 2\n",
      "Output : 0.999979\n",
      "Neuron : 3\n",
      "Output : 0.994246\n",
      "Neuron : 4\n",
      "Output : 0.996179\n",
      "Neuron : 5\n",
      "Output : 0.999952\n",
      "Neuron : 6\n",
      "Output : 0.999902\n",
      "Neuron : 7\n",
      "Output : 0.993398\n",
      "Neuron : 8\n",
      "Output : 0.999865\n",
      "Neuron : 9\n",
      "Output : 0.946698\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999983\n",
      "Target : [56]\n",
      "Input  : [7, 8]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999120\n",
      "Neuron : 1\n",
      "Output : 0.992954\n",
      "Neuron : 2\n",
      "Output : 0.999909\n",
      "Neuron : 3\n",
      "Output : 0.991080\n",
      "Neuron : 4\n",
      "Output : 0.992683\n",
      "Neuron : 5\n",
      "Output : 0.999853\n",
      "Neuron : 6\n",
      "Output : 0.999660\n",
      "Neuron : 7\n",
      "Output : 0.982083\n",
      "Neuron : 8\n",
      "Output : 0.999795\n",
      "Neuron : 9\n",
      "Output : 0.931073\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999982\n",
      "Target : [40]\n",
      "Input  : [5, 8]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999037\n",
      "Neuron : 1\n",
      "Output : 0.999550\n",
      "Neuron : 2\n",
      "Output : 0.999992\n",
      "Neuron : 3\n",
      "Output : 0.993687\n",
      "Neuron : 4\n",
      "Output : 0.997204\n",
      "Neuron : 5\n",
      "Output : 0.999966\n",
      "Neuron : 6\n",
      "Output : 0.999956\n",
      "Neuron : 7\n",
      "Output : 0.998177\n",
      "Neuron : 8\n",
      "Output : 0.999602\n",
      "Neuron : 9\n",
      "Output : 0.950522\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999983\n",
      "Target : [60]\n",
      "Input  : [10, 6]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999827\n",
      "Neuron : 1\n",
      "Output : 0.999485\n",
      "Neuron : 2\n",
      "Output : 0.999997\n",
      "Neuron : 3\n",
      "Output : 0.997456\n",
      "Neuron : 4\n",
      "Output : 0.998572\n",
      "Neuron : 5\n",
      "Output : 0.999992\n",
      "Neuron : 6\n",
      "Output : 0.999984\n",
      "Neuron : 7\n",
      "Output : 0.997839\n",
      "Neuron : 8\n",
      "Output : 0.999962\n",
      "Neuron : 9\n",
      "Output : 0.965000\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999983\n",
      "Target : [81]\n",
      "Input  : [9, 9]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999506\n",
      "Neuron : 1\n",
      "Output : 0.968407\n",
      "Neuron : 2\n",
      "Output : 0.999759\n",
      "Neuron : 3\n",
      "Output : 0.991867\n",
      "Neuron : 4\n",
      "Output : 0.990018\n",
      "Neuron : 5\n",
      "Output : 0.999793\n",
      "Neuron : 6\n",
      "Output : 0.999242\n",
      "Neuron : 7\n",
      "Output : 0.937750\n",
      "Neuron : 8\n",
      "Output : 0.999931\n",
      "Neuron : 9\n",
      "Output : 0.925863\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999981\n",
      "Target : [20]\n",
      "Input  : [2, 10]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.996448\n",
      "Neuron : 1\n",
      "Output : 0.999379\n",
      "Neuron : 2\n",
      "Output : 0.999972\n",
      "Neuron : 3\n",
      "Output : 0.986657\n",
      "Neuron : 4\n",
      "Output : 0.994569\n",
      "Neuron : 5\n",
      "Output : 0.999869\n",
      "Neuron : 6\n",
      "Output : 0.999872\n",
      "Neuron : 7\n",
      "Output : 0.997722\n",
      "Neuron : 8\n",
      "Output : 0.997793\n",
      "Neuron : 9\n",
      "Output : 0.932350\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999982\n",
      "Target : [40]\n",
      "Input  : [10, 4]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.997453\n",
      "Neuron : 1\n",
      "Output : 0.994741\n",
      "Neuron : 2\n",
      "Output : 0.999847\n",
      "Neuron : 3\n",
      "Output : 0.984869\n",
      "Neuron : 4\n",
      "Output : 0.989751\n",
      "Neuron : 5\n",
      "Output : 0.999676\n",
      "Neuron : 6\n",
      "Output : 0.999468\n",
      "Neuron : 7\n",
      "Output : 0.986423\n",
      "Neuron : 8\n",
      "Output : 0.999078\n",
      "Neuron : 9\n",
      "Output : 0.917437\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999982\n",
      "Target : [36]\n",
      "Input  : [6, 6]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.998173\n",
      "Neuron : 1\n",
      "Output : 0.956947\n",
      "Neuron : 2\n",
      "Output : 0.999157\n",
      "Neuron : 3\n",
      "Output : 0.982845\n",
      "Neuron : 4\n",
      "Output : 0.980742\n",
      "Neuron : 5\n",
      "Output : 0.999198\n",
      "Neuron : 6\n",
      "Output : 0.997792\n",
      "Neuron : 7\n",
      "Output : 0.923384\n",
      "Neuron : 8\n",
      "Output : 0.999615\n",
      "Neuron : 9\n",
      "Output : 0.899592\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999980\n",
      "Target : [16]\n",
      "Input  : [2, 8]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.991329\n",
      "Neuron : 1\n",
      "Output : 0.998651\n",
      "Neuron : 2\n",
      "Output : 0.999891\n",
      "Neuron : 3\n",
      "Output : 0.975985\n",
      "Neuron : 4\n",
      "Output : 0.989546\n",
      "Neuron : 5\n",
      "Output : 0.999546\n",
      "Neuron : 6\n",
      "Output : 0.999592\n",
      "Neuron : 7\n",
      "Output : 0.995789\n",
      "Neuron : 8\n",
      "Output : 0.993610\n",
      "Neuron : 9\n",
      "Output : 0.910556\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999982\n",
      "Target : [27]\n",
      "Input  : [9, 3]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.946177\n",
      "Neuron : 1\n",
      "Output : 0.966701\n",
      "Neuron : 2\n",
      "Output : 0.991830\n",
      "Neuron : 3\n",
      "Output : 0.902305\n",
      "Neuron : 4\n",
      "Output : 0.929821\n",
      "Neuron : 5\n",
      "Output : 0.985174\n",
      "Neuron : 6\n",
      "Output : 0.984474\n",
      "Neuron : 7\n",
      "Output : 0.944264\n",
      "Neuron : 8\n",
      "Output : 0.958465\n",
      "Neuron : 9\n",
      "Output : 0.813068\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999973\n",
      "Target : [8]\n",
      "Input  : [4, 2]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.994303\n",
      "Neuron : 1\n",
      "Output : 0.846749\n",
      "Neuron : 2\n",
      "Output : 0.993105\n",
      "Neuron : 3\n",
      "Output : 0.961905\n",
      "Neuron : 4\n",
      "Output : 0.949975\n",
      "Neuron : 5\n",
      "Output : 0.995137\n",
      "Neuron : 6\n",
      "Output : 0.987011\n",
      "Neuron : 7\n",
      "Output : 0.797041\n",
      "Neuron : 8\n",
      "Output : 0.998620\n",
      "Neuron : 9\n",
      "Output : 0.852320\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999971\n",
      "Target : [0]\n",
      "Input  : [0, 7]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.996752\n",
      "Neuron : 1\n",
      "Output : 0.990309\n",
      "Neuron : 2\n",
      "Output : 0.999680\n",
      "Neuron : 3\n",
      "Output : 0.981201\n",
      "Neuron : 4\n",
      "Output : 0.985848\n",
      "Neuron : 5\n",
      "Output : 0.999429\n",
      "Neuron : 6\n",
      "Output : 0.999008\n",
      "Neuron : 7\n",
      "Output : 0.977705\n",
      "Neuron : 8\n",
      "Output : 0.998863\n",
      "Neuron : 9\n",
      "Output : 0.906461\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999981\n",
      "Target : [30]\n",
      "Input  : [5, 6]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.997843\n",
      "Neuron : 1\n",
      "Output : 0.984811\n",
      "Neuron : 2\n",
      "Output : 0.999641\n",
      "Neuron : 3\n",
      "Output : 0.983888\n",
      "Neuron : 4\n",
      "Output : 0.985941\n",
      "Neuron : 5\n",
      "Output : 0.999490\n",
      "Neuron : 6\n",
      "Output : 0.998916\n",
      "Neuron : 7\n",
      "Output : 0.967311\n",
      "Neuron : 8\n",
      "Output : 0.999404\n",
      "Neuron : 9\n",
      "Output : 0.908907\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999981\n",
      "Target : [28]\n",
      "Input  : [4, 7]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999612\n",
      "Neuron : 1\n",
      "Output : 0.982679\n",
      "Neuron : 2\n",
      "Output : 0.999885\n",
      "Neuron : 3\n",
      "Output : 0.993468\n",
      "Neuron : 4\n",
      "Output : 0.992779\n",
      "Neuron : 5\n",
      "Output : 0.999882\n",
      "Neuron : 6\n",
      "Output : 0.999594\n",
      "Neuron : 7\n",
      "Output : 0.961477\n",
      "Neuron : 8\n",
      "Output : 0.999944\n",
      "Neuron : 9\n",
      "Output : 0.934727\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999982\n",
      "Target : [30]\n",
      "Input  : [3, 10]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.868988\n",
      "Neuron : 1\n",
      "Output : 0.712139\n",
      "Neuron : 2\n",
      "Output : 0.862591\n",
      "Neuron : 3\n",
      "Output : 0.792488\n",
      "Neuron : 4\n",
      "Output : 0.782027\n",
      "Neuron : 5\n",
      "Output : 0.874063\n",
      "Neuron : 6\n",
      "Output : 0.839477\n",
      "Neuron : 7\n",
      "Output : 0.692151\n",
      "Neuron : 8\n",
      "Output : 0.908741\n",
      "Neuron : 9\n",
      "Output : 0.715575\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999887\n",
      "Target : [0]\n",
      "Input  : [0, 2]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.999641\n",
      "Neuron : 1\n",
      "Output : 0.996746\n",
      "Neuron : 2\n",
      "Output : 0.999977\n",
      "Neuron : 3\n",
      "Output : 0.995078\n",
      "Neuron : 4\n",
      "Output : 0.996204\n",
      "Neuron : 5\n",
      "Output : 0.999957\n",
      "Neuron : 6\n",
      "Output : 0.999893\n",
      "Neuron : 7\n",
      "Output : 0.990247\n",
      "Neuron : 8\n",
      "Output : 0.999929\n",
      "Neuron : 9\n",
      "Output : 0.948152\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999983\n",
      "Target : [54]\n",
      "Input  : [6, 9]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.983662\n",
      "Neuron : 1\n",
      "Output : 0.881170\n",
      "Neuron : 2\n",
      "Output : 0.988539\n",
      "Neuron : 3\n",
      "Output : 0.936676\n",
      "Neuron : 4\n",
      "Output : 0.931116\n",
      "Neuron : 5\n",
      "Output : 0.989360\n",
      "Neuron : 6\n",
      "Output : 0.979844\n",
      "Neuron : 7\n",
      "Output : 0.838852\n",
      "Neuron : 8\n",
      "Output : 0.993820\n",
      "Neuron : 9\n",
      "Output : 0.826017\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999971\n",
      "Target : [5]\n",
      "Input  : [1, 5]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.993277\n",
      "Neuron : 1\n",
      "Output : 0.941580\n",
      "Neuron : 2\n",
      "Output : 0.997054\n",
      "Neuron : 3\n",
      "Output : 0.964175\n",
      "Neuron : 4\n",
      "Output : 0.963168\n",
      "Neuron : 5\n",
      "Output : 0.996902\n",
      "Neuron : 6\n",
      "Output : 0.993587\n",
      "Neuron : 7\n",
      "Output : 0.906035\n",
      "Neuron : 8\n",
      "Output : 0.997866\n",
      "Neuron : 9\n",
      "Output : 0.865364\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999977\n",
      "Target : [12]\n",
      "Input  : [2, 6]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.963634\n",
      "Neuron : 1\n",
      "Output : 0.948506\n",
      "Neuron : 2\n",
      "Output : 0.990853\n",
      "Neuron : 3\n",
      "Output : 0.915297\n",
      "Neuron : 4\n",
      "Output : 0.930255\n",
      "Neuron : 5\n",
      "Output : 0.986724\n",
      "Neuron : 6\n",
      "Output : 0.983061\n",
      "Neuron : 7\n",
      "Output : 0.919563\n",
      "Neuron : 8\n",
      "Output : 0.977815\n",
      "Neuron : 9\n",
      "Output : 0.817465\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999973\n",
      "Target : [9]\n",
      "Input  : [3, 3]\n",
      "==================================================\n",
      "\n",
      "\n",
      "------ \n",
      "Neurons: 10\n",
      "Neuron : 0\n",
      "Output : 0.926449\n",
      "Neuron : 1\n",
      "Output : 0.995968\n",
      "Neuron : 2\n",
      "Output : 0.998511\n",
      "Neuron : 3\n",
      "Output : 0.912988\n",
      "Neuron : 4\n",
      "Output : 0.961720\n",
      "Neuron : 5\n",
      "Output : 0.993952\n",
      "Neuron : 6\n",
      "Output : 0.996221\n",
      "Neuron : 7\n",
      "Output : 0.990304\n",
      "Neuron : 8\n",
      "Output : 0.905918\n",
      "Neuron : 9\n",
      "Output : 0.843619\n",
      "================== Output Layer ==================\n",
      "Neurons: 1\n",
      "Neuron : 0\n",
      "Output : 0.999976\n",
      "Target : [0]\n",
      "Input  : [8, 0]\n",
      "==================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RUN ME!\n",
    "# Build the training set\n",
    "\n",
    "N = 20 # number of training sets\n",
    "V = 10 # range of values \n",
    "trainingData = []\n",
    "\n",
    "for i in range(N):\n",
    "    first = randint(0, V)\n",
    "    second = randint(0, V)\n",
    "    answer = first * second\n",
    "    \n",
    "    # take the log of everything, so the answer is an addition problem\n",
    "    trainingData.append([[first, second], [answer]])\n",
    "\n",
    "# Choose an activation function \n",
    "activation = \"sigmoid\"\n",
    "\n",
    "# Choose number of inputs, hidden layers and outputs\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=10,num_outputs=1)\n",
    "\n",
    "# Choose number of iterations\n",
    "num_iterations = 10\n",
    "\n",
    "# Train the NN\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1]) \n",
    "\n",
    "# Feed the NN input to produce output\n",
    "NeuralNetwork.test_cases(myNN, trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the output is very close to zero (0.01) and very close to one (0.98) when at the right times.\n",
    "# Hurray! We have successfully trained the Neural Network to learn the XOR algorithm using fundamental concepts \n",
    "# such as back propogation, learning rates and activation functions.\n",
    "#\n",
    "# Below, we move on to plotting error rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error over 1000 iterations:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njyj0RUTyiEJfRCSP/D/Bl03dSU7Y6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57f773ebd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Data and training (steps are same as above)\n",
    "trainingData = [[[1,1], [0]],\n",
    "                [[1,0], [1]],\n",
    "                [[0,1], [1]],\n",
    "                [[0,0], [0]]]\n",
    "activation = \"sigmoid\"\n",
    "myNN = NeuralNetwork(num_inputs=2,num_hidden=2,num_outputs=1)\n",
    "num_iterations = 1000\n",
    "\n",
    "average_errors = [] # an array of 1000 errors, corresponding to number of iterations\n",
    "for i in range(num_iterations):\n",
    "    for k in range(len(trainingData)):\n",
    "        NeuralNetwork.train(myNN, inputs=trainingData[k][0],target=trainingData[k][1])\n",
    "    average_error = NeuralNetwork.calculate_total_error(myNN, trainingData) / len(trainingData) \n",
    "    average_errors.append(average_error)\n",
    "\n",
    "plot_learning_curves(np.arange(num_iterations), average_errors)\n",
    "print (\"Error over %d iterations:\" % num_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# if a sharp 'L' shaped graph appears, please re-run the code. It should look sharp AND THEN decreasing.\n",
    "#\n",
    "# OBSERVATIONS:\n",
    "# We see that as the number of iterations increase (x-axis), the average error decreases (y-axis). The sharpest \n",
    "# error reduction happens in the earliest iterations. Then, it decreases at a gradual but not at a uniform pace. \n",
    "# As the code is re-run, the shape of the graph changes. Sometimes, the error is dramatically reduced within the \n",
    "# first few iterations, creating a long, almost vertical line in the beginning. Other times, the initial error \n",
    "# reduction is not that great, and error reduction is more gradual throughout the iterations. These observations \n",
    "# can be explained by the random assignment of weights at the beginning. Sometimes, the weights can be randomly \n",
    "# guessed more 'correctly' than others, resulting in a more accurate output and hence produce lower error rates.\n",
    "# \n",
    "# SIDE NOTE:\n",
    "# The author learnt a lot about how learning rates can have a huge effect on the NN's accuracy. When the learning\n",
    "# rate was set low, such as 0.1, the NN was very innacurate. This is becuase low learning rates do not 'punish'\n",
    "# errors harshly enough during back-propogation. When the learning rate was set higher, such as 0.5, the NN\n",
    "# works well and is able to achieve near-perfect results. The downside to this is the possibility of overfitting, \n",
    "# but since XOR is limited to a relatively simple exercise with just four possible test cases, this is not \n",
    "# a cause of concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
